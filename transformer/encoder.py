"""
HiDiNet: Hierarchical Diffusion Network for Healthcare Time Series Modeling
Transformer Encoder Component

This module implements the transformer encoder component of the HiDiNet architecture.
The encoder processes health trajectory sequences to capture long-range temporal 
dependencies and complex patterns in disease progression.

Key Features:
- Processes combined health trajectories and time information
- Uses multi-head self-attention to capture variable interactions over time
- Applies positional encoding to maintain temporal ordering
- Generates rich contextual representations for downstream survival prediction

Architecture:
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│  Visit Matrix   │ -> │ Input Embedding  │ -> │ Positional Enc. │
│ [Health + Time] │    │ (Linear Proj.)   │    │ (Sin/Cos)       │
└─────────────────┘    └──────────────────┘    └─────────────────┘
                                                        │
                                                        v
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│ Encoded Output  │ <- │ Transformer      │ <- │ Input + Pos.    │
│ [B, T, d_model] │    │ Encoder Layers   │    │ [B, T, d_model] │
└─────────────────┘    └──────────────────┘    └─────────────────┘

The encoder takes health trajectories generated by the SDE dynamics model and
produces contextual representations that capture both local and global temporal
patterns for survival analysis.

Authors: [Your names here]
Paper: [Paper title and venue]
"""

import torch
import torch.nn as nn
import math

class Encoder(nn.Module):
    """
    Transformer Encoder for Health Trajectory Processing
    
    This encoder processes sequences of health measurements combined with time information
    to generate rich contextual representations. It uses the standard transformer encoder
    architecture with multi-head self-attention and feed-forward networks.
    
    The encoder is designed to handle:
    - Variable-length health trajectories (though padded to fixed length)
    - Missing or irregular observations (handled upstream)
    - Complex temporal dependencies between health variables
    - Long-range patterns in disease progression
    
    Architecture Details:
    - Input: Visit matrix [B, 50, 30] containing 29 health vars + 1 time var
    - Embedding: Linear projection to d_model dimensions
    - Positional Encoding: Standard sinusoidal encoding for temporal order
    - Transformer Layers: 4 encoder layers with 8 attention heads each
    - Output: Encoded representations [B, 50, d_model]
    
    Args:
        device (torch.device): Computing device (CPU/GPU)
        d_model (int): Model dimension (typically 512)
        dropout (float): Dropout rate for regularization
    """
    
    def __init__(self, device, d_model, dropout):
        super(Encoder, self).__init__()
        
        # ==================== CONFIGURATION ====================
        
        self.device = device
        self.d_model = d_model      # Transformer model dimension
        self.dropout = dropout      # Dropout rate for regularization

        # ==================== INPUT EMBEDDING ====================
        
        # Linear projection from input features to model dimension
        # Input: 30 features (29 health variables + 1 time variable)
        # Output: d_model dimensional embeddings
        self.input_embedding = nn.Linear(30, d_model)
        
        # Note: Alternative architecture with separate embeddings was considered:
        # - Health variables: 29 features -> d_model
        # - Background variables: 14 features -> d_model  
        # - Medication variables: 5 features -> d_model
        # - Time variables: 1 feature -> d_model
        # - Combined: 4*d_model -> d_model
        # Current simpler approach works better in practice

        # ==================== POSITIONAL ENCODING ====================
        
        # Standard sinusoidal positional encoding to maintain temporal order
        # This is crucial for transformers to understand sequence ordering
        self.positional_encoding = self.PositionalEncoding(d_model, dropout)

        # ==================== TRANSFORMER ENCODER ====================
        
        # Single transformer encoder layer configuration
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,                    # Model dimension
            nhead=8,                           # Number of attention heads (8 is standard)
            dim_feedforward=d_model*4,         # Feed-forward network size (4x expansion)
            dropout=dropout,                   # Dropout for regularization
            batch_first=True,                  # Input shape: [batch, seq_len, features]
            activation='relu'                  # Activation function in feed-forward network
        )

        # Stack multiple encoder layers for deeper representations
        # 4 layers provides good balance between capacity and computational cost
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=4
        )

    def forward(self, visit_matrix, times):
        """
        Forward pass through the transformer encoder.
        
        This method processes health trajectory sequences to generate contextual
        representations that capture both local and global temporal patterns.
        
        Processing Pipeline:
        1. Linear embedding of input features to model dimension
        2. Addition of positional encodings for temporal order
        3. Multi-layer transformer encoding with self-attention
        4. Output of rich contextual representations
        
        Args:
            visit_matrix (torch.Tensor): [B, 50, 30] tensor containing:
                - First 29 dimensions: Health variables (e.g., blood pressure, BMI, etc.)
                - Last 1 dimension: Time variable (normalized age)
            times (torch.Tensor): [B, 50, 1] tensor of timepoints (for compatibility,
                                 time info is already included in visit_matrix)
        
        Returns:
            torch.Tensor: [B, 50, d_model] encoded sequence representations
                         Each timestep now contains rich contextual information
                         about the patient's health trajectory
        """
        
        # ==================== INPUT VALIDATION ====================
        
        batch_size = visit_matrix.shape[0]
        
        # Validate input tensor shapes
        assert visit_matrix.shape == (batch_size, 50, 30), \
            f"Expected visit_matrix shape [B, 50, 30], got {visit_matrix.shape}"
        
        # ==================== INPUT EMBEDDING ====================
        
        # Project input features to model dimension
        # This creates initial embeddings that the transformer can work with
        x = self.input_embedding(visit_matrix)  # [B, 50, d_model]
        
        # Validate embedding output shape
        assert x.shape == (batch_size, 50, self.d_model), \
            f"Expected embedding shape [B, 50, {self.d_model}], got {x.shape}"
        
        # ==================== POSITIONAL ENCODING ====================
        
        # Add positional encodings to maintain temporal order information
        # Transformers are permutation-invariant without positional encoding
        x = self.positional_encoding(x)  # [B, 50, d_model]
        
        # Validate positional encoding output shape
        assert x.shape == (batch_size, 50, self.d_model), \
            f"Expected pos encoding shape [B, 50, {self.d_model}], got {x.shape}"
        
        # ==================== TRANSFORMER ENCODING ====================
        
        # Pass through multi-layer transformer encoder
        # Each layer applies:
        # 1. Multi-head self-attention (captures relationships between timesteps)
        # 2. Feed-forward network (non-linear transformations)
        # 3. Residual connections and layer normalization
        encoded = self.transformer_encoder(x)  # [B, 50, d_model]
        
        # Validate final output shape
        assert encoded.shape == (batch_size, 50, self.d_model), \
            f"Expected encoded shape [B, 50, {self.d_model}], got {encoded.shape}"
        
        return encoded

    class PositionalEncoding(nn.Module):
        """
        Sinusoidal Positional Encoding for Transformer
        
        This class implements the standard sinusoidal positional encoding used in
        the original "Attention Is All You Need" paper. It adds position information
        to the input embeddings so the transformer can understand sequence order.
        
        The encoding uses sine and cosine functions of different frequencies:
        - PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
        - PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
        
        This creates unique encodings for each position that the model can learn to use.
        
        Args:
            d_model (int): Model dimension
            dropout (float): Dropout rate applied after adding positional encoding
            max_len (int): Maximum sequence length supported (default: 5000)
        """
        
        def __init__(self, d_model, dropout, max_len=5000):
            super().__init__()
            
            # Dropout layer applied after adding positional encodings
            self.dropout = nn.Dropout(dropout)
            
            # ==================== POSITIONAL ENCODING COMPUTATION ====================
            
            # Create positional encoding matrix [max_len, d_model]
            pe = torch.zeros(max_len, d_model)
            
            # Create position indices [max_len, 1]
            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
            
            # Create frequency terms for sine/cosine functions
            # This creates exponentially decreasing frequencies
            div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                                (-math.log(10000.0) / d_model))
            
            # Apply sine to even indices (0, 2, 4, ...)
            pe[:, 0::2] = torch.sin(position * div_term)
            
            # Apply cosine to odd indices (1, 3, 5, ...)
            # Handle case where d_model is odd
            if d_model % 2 == 0:
                pe[:, 1::2] = torch.cos(position * div_term)
            else:
                pe[:, 1::2] = torch.cos(position * div_term[:-1])
            
            # Register as buffer (not a parameter, but part of model state)
            # This ensures it moves with the model to different devices
            self.register_buffer('pe', pe)

        def forward(self, x):
            """
            Add positional encodings to input embeddings.
            
            Args:
                x (torch.Tensor): [B, seq_len, d_model] input embeddings
                
            Returns:
                torch.Tensor: [B, seq_len, d_model] input with positional encodings added
            """
            
            # Get sequence length from input
            seq_len = x.size(1)
            
            # Add positional encoding (broadcast across batch dimension)
            # Only use the first seq_len positions from the pre-computed encodings
            x = x + self.pe[:seq_len, :].unsqueeze(0)  # [B, seq_len, d_model]
            
            # Apply dropout for regularization
            return self.dropout(x)